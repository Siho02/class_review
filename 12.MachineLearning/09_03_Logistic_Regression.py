'''
ë¡œì§€ìŠ¤í‹± íšŒê·€(Logistic Regression)
    - ì„ í˜• íšŒê·€ ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•œ ì´ì§„ ë¶„ë¥˜ ëª¨ë¸
    - sampleì´ íŠ¹ì • í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ì¶”ì •

    1. í™•ë¥  ì¶”ì • 
        - ì„ í˜• íšŒê·€ì²˜ëŸ¼ ì…ë ¥íŠ¹ì„±(feature)ì— ê°€ì¤‘ì¹˜ í•©ì„ ê³„ì‚°í•œ ê°’ì„ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ë¥¼ ì ìš©í•´ í™•ë¥ ì„ ê³„ì‚°

        1) ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜
            - 0ê³¼ 1 ì‚¬ì´ì˜ ì‹¤ìˆ˜ë¥¼ ë°˜í™˜
            - Sì í˜•íƒœì˜ ê²°ê³¼ë¥¼ ë‚´ëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì´ë‹¤.
                ğœ(ğ‘¥)= 1 / (1 + ğ^(-x))
            - ìƒ˜í”Œ xê°€ ì–‘ì„±ì— ì†í•  í™•ë¥ 
                y_hat = 0 (p_hat < 0.5)
                        1 (p_hat < 0.5) 
    
    2. ì†ì‹¤ í•¨ìˆ˜(loss function)
        - ì†ì‹¤ í•¨ìˆ˜ L(W) = -(1/m) * âˆ‘[y_i * log(p_i_hat) + (1 - y_i) * log(1 - p_i_hat)]
        - y(ì‹¤ì œê°’)ì´ 1ì¸ ê²½ìš° y_i * log(p_i_hat)ì´ ì†ì‹¤ì„ ê³„ì‚°
        - yê°€ 0ì¸ ê²½ìš° (1 - y_i) * log(1 - p_i_hat)ì´ ì†ì‹¤ì„ ê³„ì‚°
        - p_hat(ì˜ˆì¸¡í™•ë¥ )ì´ í´ìˆ˜ë¡ ë°˜í™˜ê°’ì€ ì‘ì•„ì§€ê³ , ì‘ì„ ìˆ˜ë¡ ê°’ì´ ì»¤ì§„ë‹¤
    
    3. ìµœì í™”
        - ìœ„ ì†ì‹¤ì„ ê°€ì¥ ì ê²Œí•˜ëŠ” ê°€ì¤‘ì¹˜(W)ë¥¼ ì°¾ëŠ”ë‹¤
        - ë¡œê·¸ ì†ì‹¤í•¨ìˆ˜ëŠ” ìµœì†Œê°’ì„ ì°¾ëŠ” ì •ê·œë°©ì •ì‹ì´ ì—†ìŒ >> Logistic Regressionì€ ê²½ì‚¬í•˜ê°•ë²•ì„ ì´ìš©í•´ ìµœì í™”
        - ë¡œê·¸ ì†ì‹¤ì„ Wë¡œ ë¯¸ë¶„ >> (1 / m) * âˆ‘(Ïƒ(W^T * x_i) - y_i)x_ij   
    
    4. ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°
        - penalty : ê³¼ì í•©ì„ ì¤„ì´ê¸° ìœ„í•œ ê·œì œ ë°©ì‹
            - l1, l2(ê¸°ë³¸ê°’), elasticnet, none
        - C : ê·œì œê°•ë„(ê¸°ë³¸ê°’=1), ì‘ì„ ìˆ˜ë¡ ê·œì œê°€ ê°•í•˜ë‹¤
        - max_iter(ê¸°ë³¸ê°’=100) : ê²½ì‚¬í•˜ê°•ë²• ë°˜ë³µ íšŸìˆ˜
'''
## ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜
import matplotlib.pyplot as plt
import numpy as np

xx = np.linspace(-10, 10, 100)      #shape = (100, )
sigmoid = 1 / (1 + np.exp(-xx))     #shape = (100, )

plt.figure(figsize=(12, 6))
plt.plot(xx, sigmoid, color='b', linewidth = 2)
plt.plot([-10, 10], [0, 0], color = 'k', linestyle = '-')
plt.plot([-10, 10], [0.5, 0.5], color='r', linestyle=':', label='y=0.5')
plt.xlabel("x")
plt.legend()
plt.xlim(-10, 10)
plt.ylim(-0.1, 1.1)
plt.grid(True)
plt.show()

## ì†ì‹¤í•¨ìˆ˜
print('logê³„ì‚°')
print('-log(1), -log(0.99),\t-log(0.7),\t-log(0.51),\t-log(0.5),\t-log(0.4),\t-log(0.2),\t-log(0.0000001)')
print(-np.log(1), -np.log(0.99), -np.log(0.7), -np.log(0.51), -np.log(0.5), -np.log(0.4), -np.log(0.2), -np.log(0.0000001))
print('\nnp.log(1), np.log(0.7), np.log(0.5), np.log(0.1), np.log(0.0000000001)')
print(np.log(1), np.log(0.7), np.log(0.5), np.log(0.1), np.log(0.0000000001))

from sklearn.datasets import load_breast_cancer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import pandas as pd

#ë°ì´í„° load, split
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)

#scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ëª¨ë¸ ìƒì„± + í•™ìŠµ
lr = LogisticRegression(random_state=0)
lr.fit(X_train_scaled, y_train)

# í‰ê°€
pred_train = lr.predict(X_train_scaled)
pred_test = lr.predict(X_test_scaled)
accuracy_score(y_train, pred_train),  accuracy_score(y_test, pred_test)

# GridSearchCV íŒŒë¼ë¯¸í„° - penalty, C 
param = {
    'penalty':['l1', 'l2'], 
    'C':[0.001, 0.01, 0.1, 1, 10]
}

gs = GridSearchCV(LogisticRegression(random_state=0), 
                  param,
                  cv=5, 
                  scoring='accuracy', 
                  n_jobs=-1)

gs.fit(X_train_scaled, y_train)

result = pd.DataFrame(gs.cv_results_)
print(result.sort_values('rank_test_score').head())
print(gs.best_params_)

best_model = gs.best_estimator_
pred_test = best_model.predict(X_test_scaled)
print(accuracy_score(y_test, pred_test))