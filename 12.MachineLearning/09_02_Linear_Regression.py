'''
íšŒê·€ ëª¨ë¸(Regression Model)

1. ì„ í˜• íšŒê·€(Linear Regression)
    - ì¢…ì† ë³€ìˆ˜ yì™€ í•œê°œ ì´ìƒì˜ ë…ë¦½ ë³€ìˆ˜(ë˜ëŠ” ì„¤ëª… ë³€ìˆ˜) Xì™€ì˜ ì„ í˜• ìƒê´€ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” íšŒê·€ ë¶„ì„ ê¸°ë²•
    
    1) ê°€ì¥ ê¸°ë³¸ì ì¸ ì„ í˜• íšŒê·€ ëª¨ë¸
    2) feture ì „ì²˜ë¦¬
        (1) ë²”ì£¼í˜• ë°ì´í„° : OneHotEncoder
        (2) ì—°ì†í˜• ë°ì´í„° : Feature scaling(standard scalerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì´ ë” ì˜ ë‚˜ì˜¤ëŠ” ê²½í–¥ì´ ìˆë‹¤)

2. ì„ í˜• íšŒê·€ ëª¨ë¸ : y_i_hat = w1*xi1 + w2*xi2 + .... + wp*xip + b
                    (y_i_hat : ì˜ˆì¸¡ê°’ / x : íŠ¹ì„± / w : ê°€ì¤‘ì¹˜ / b : ì ˆí¸ / p : pë²ˆì§¸ íŠ¹ì„± / i : ië²ˆì§¸ íŠ¹ì„±)

3. ë‹¤í•­ íšŒê·€(Polynomial Regression)
    - ë‹¨ìˆœí•œ ì§ì„ í˜•ë³´ë‹¤ëŠ” ë³µì¡í•œ ë¹„ì„ í˜• í˜•íƒœì˜ ë°ì´í„°ë¥¼ ì¶”ë¡ í•˜ê¸° ìœ„í•œ ëª¨ë¸
    - featureë“¤ì„ ê±°ë“­ì œê³±í•œ featureë“¤ì„ ì¶”ê°€í•˜ì—¬ ëª¨ë¸ë§
    - PolynomialFeatures ë³€í™˜ê¸°ë¥¼ ì´ìš©

4. ì†ì‹¤(loss) í•¨ìˆ˜
    1) ëª¨ë¸ì´ ì¶œë ¥í•œ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    2) í‰ê°€ ì§€í‘œë¡œ ì‚¬ìš©ë˜ê¸°ë„ í•˜ê³  ëª¨ë¸ì„ ìµœì í™”í•˜ëŠ”ë° ì‚¬ìš©
    3) ì˜¤ì°¨í•¨ìˆ˜(error), ë¹„ìš©í•¨ìˆ˜(cost), ëª©ì í•¨ìˆ˜(objective)ë¼ê³ ë„ í•¨

5. ìµœì í™”(optimize)
    1) ì •ì˜ : ì†ì‹¤í•¨ìˆ˜ì˜ ê°’ì´ ìµœì†Œí™” ë˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ê³¼ì •
    2) ë‘ê°€ì§€ ë°©ë²•
        (1) ì •ê·œ ë°©ì •ì‹
        (2) ê²½ì‚¬ í•˜ê°•ë²•

'''

#Boston dataë¥¼ ì´ìš©í•˜ê¸°
# CRIM : ì§€ì—­ë³„ ë²”ì£„ ë°œìƒë¥  / ZN : 25,000 í‰ë°©í”¼íŠ¸ë¥¼ ì´ˆê³¼í•˜ëŠ” ê±°ì£¼ì§€ì—­ì˜ ë¹„ìœ¨ / INDUS: ë¹„ìƒì—…ì§€ì—­ í† ì§€ì˜ ë¹„ìœ¨
# CHAS : ì°°ìŠ¤ê°•ì— ëŒ€í•œ ë”ë¯¸ë³€ìˆ˜(ê°•ì˜ ê²½ê³„ì— ìœ„ì¹˜í•œ ê²½ìš°ëŠ” 1, ì•„ë‹ˆë©´ 0) / NOX : ì¼ì‚°í™”ì§ˆì†Œ ë†ë„ / RM : ì£¼íƒ 1ê°€êµ¬ë‹¹ í‰ê·  ë°©ì˜ ê°œìˆ˜
# AGE : 1940ë…„ ì´ì „ì— ê±´ì¶•ëœ ì†Œìœ ì£¼íƒì˜ ë¹„ìœ¨ / DIS : 5ê°œì˜ ë³´ìŠ¤í„´ ê³ ìš©ì„¼í„°ê¹Œì§€ì˜ ì ‘ê·¼ì„± ì§€ìˆ˜ / RAD : ê³ ì†ë„ë¡œê¹Œì§€ì˜ ì ‘ê·¼ì„± ì§€ìˆ˜
# TAX : 10,000 ë‹¬ëŸ¬ ë‹¹ ì¬ì‚°ì„¸ìœ¨ /PTRATIO : ì§€ì—­ë³„ êµì‚¬ í•œëª…ë‹¹ í•™ìƒ ë¹„ìœ¨ / B : ì§€ì—­ì˜ í‘ì¸ ê±°ì£¼ ë¹„ìœ¨ / LSTAT: í•˜ìœ„ê³„ì¸µì˜ ë¹„ìœ¨(%)
# MEDV : Target. ë³¸ì¸ ì†Œìœ ì˜ ì£¼íƒê°€ê²©(ì¤‘ì•™ê°’) (ë‹¨ìœ„: $1,000)

import pandas as pd
import numpy as np
from sklearn.datasets import load_boston
data = load_boston()
data.keys()
    # dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])

X, y = data['data'], data['target']
df = pd.DataFrame(X, columns=data['feature_names'])
print(df)
df['MEDV'] = y

#print(df['CHAS'].value_counts()) #ë²”ì£¼í˜•
    #0.0    471
    #1.0     35
    #Name: CHAS, dtype: int64

#Linear Regression
    # ë²”ì£¼í˜•(CHAS) : onehotencoding
    # ì—°ì†í˜•(ë‚˜ë¨¸ì§€) : standard scaling

#CHASë°ì´í„° ë°›ì•„ ë”ë¯¸ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
chas_df = pd.get_dummies(df['CHAS'])
chas_df.columns = ['CHAS_0', 'CHAS_1']

df2 = df.join(chas_df) #CHAS ì›í•«ì¸ì½”ë”© ì»¬ëŸ¼ì„ DFì— ì¶”ê°€
df2.drop(columns='CHAS', inplace=True) # ì›ë˜ ìˆë˜ CHASì»¬ëŸ¼ ì œê±°
    #      CRIM    ZN  INDUS    NOX     RM   AGE     DIS  RAD    TAX  PTRATIO       B  LSTAT  MEDV  CHAS_0  CHAS_1
    #0  0.00632  18.0   2.31  0.538  6.575  65.2  4.0900  1.0  296.0     15.3  396.90   4.98  24.0       1       0
    #1  0.02731   0.0   7.07  0.469  6.421  78.9  4.9671  2.0  242.0     17.8  396.90   9.14  21.6       1       0
    #2  0.02729   0.0   7.07  0.469  7.185  61.1  4.9671  2.0  242.0     17.8  392.83   4.03  34.7       1       0
    #3  0.03237   0.0   2.18  0.458  6.998  45.8  6.0622  3.0  222.0     18.7  394.63   2.94  33.4       1       0
    #4  0.06905   0.0   2.18  0.458  7.147  54.2  6.0622  3.0  222.0     18.7  396.90   5.33  36.2       1       0

y = df2['MEDV'] #target data
X = df2.drop(columns='MEDV')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
scaler = StandardScaler()
X_trian_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
    # X_train.columns = Index(['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 
    #                           'CHAS_0', 'CHAS_1'], dtype='object')

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from util import print_metrics, print_regression_metrics

lr = LinearRegression()
lr.fit(X_trian_scaled, y_train)

print('linear regressionì˜ intercept, coefficient')
print(lr.intercept_, lr.coef_)

# lr.coef_ë¥¼ ì—°ê´€ì§€ì–´ ì‹œë¦¬ì¦ˆë¡œ êµ¬ì„±
pd.Series(lr.coef_, index = X_train.columns)

# ëª¨ë¸ì„ í†µí•œ ì˜ˆì¸¡
pred_train = lr.predict(X_trian_scaled)
pred_test = lr.predict(X_test_scaled)
print_regression_metrics(y_train, pred_train, title='Train')
print_regression_metrics(y_test, pred_test, title='Test')

# ì‹¤ì œê°’(MEDV)ì™€ ì˜ˆì¸¡ê°’ì„ pyplotì„ ì´ìš©í•˜ì—¬ ë¹„êµí•´ë³´ê¸°
import matplotlib.pyplot as plt
plt.figure(figsize=(12, 6))
plt.plot(range(len(y_test)), y_test, label='MEDV', marker='o')
plt.plot(range(len(y_test)), pred_test, label='Pred', marker='x')
plt.legend()
plt.show()


###################### ë‹¤í•­ íšŒê·€ ##########################
np.random.seed(0)
m = 100
X = 6 * np.random.rand(m, 1) - 3
y = X ** 2 + X + 2 + np.random.normal(0,1, size=(m,1))
y = y.flatten()

df = pd.DataFrame({"X" : X.flatten(), "Y" : y})
'''
          X         Y
0  0.292881  1.213510
1  1.291136  5.858995
2  0.616580  3.462414
3  0.269299  0.805577
4 -0.458071  3.240010
'''

#ê·¸ë˜í”„ë¡œ í‘œí˜„
plt.figure(figsize=(7,6))
plt.scatter(X, y, alpha=0.5)
plt.show()

lr = LinearRegression()
lr.fit(X, y)
print(lr.coef_, lr.intercept_)
    # (array([0.78189543]), 5.175619278567209)

pred = lr.predict(X)
print_regression_metrics(y, pred)

X_new = np.linspace(-3, 3, 100).reshape(-1,1)
#print(X_new)
y_new = lr.predict(X_new)

plt.figure(figsize=(7,6))
plt.scatter(X, y, alpha=0.5)
plt.plot(X_new, y_new, color='red')
plt.show()
    # X, yì˜ ê´€ê³„ë¥¼ ì •í™•í•˜ê²Œ ì„¤ëª…í•  ìˆ˜ X

## Xì˜ featureìˆ˜ë¥¼ ëŠ˜ë ¤ ë‹¤í•­ì‹ì´ ë˜ë„ë¡ ì²˜ë¦¬(2ì°¨í•¨ìˆ˜) >> polynomial features
from sklearn.preprocessing import PolynomialFeatures

pnf = PolynomialFeatures(degree=2, include_bias=False)
    #degree : ìµœê³ ì°¨í•­ ì§€ì • / include_bias=False : ìƒìˆ˜í•­ì„ ì¶”ê°€(ëª¨ë“ ê°’ 1ì¸ feature ì¶”ê°€)
X_poly = pnf.fit_transform(X)
    #X.shape = (100, 1) >> X_poly.shape = (100, 2)
print(pnf.get_feature_names())

lr2 = LinearRegression()
lr2.fit(X_poly, y)
print('lr2ì˜ coef, intercept', lr2.coef_, lr2.intercept_)

##ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
X_new_poly = pnf.transform(X_new)
y_new = lr2.predict(X_new_poly)

plt.figure(figsize=(6,6))
plt.scatter(X, y, alpha=0.5)
plt.plot(X_new, y_new, color='red')
plt.show()

print('\n============lrê³¼ lr2ì˜ í‰ê°€ì§€í‘œ============')
print_regression_metrics(y, lr.predict(X))
print_regression_metrics(y, lr2.predict(X_poly))
print()

##input dataê°€ ë‹¤ì°¨ì›(feature ìˆ˜ = 3)
data = np.arange(12).reshape(4,3) 
    #array([[ 0,  1,  2],
    #       [ 3,  4,  5],
    #       [ 6,  7,  8],
    #       [ 9, 10, 11]])

pnf2 = PolynomialFeatures(degree=2)
data_poly = pnf2.fit_transform(data)
    #shape = (4, 10)

pnf2.get_feature_names()
    #['1', 'x0', 'x1', 'x2', 'x0^2', 'x0 x1', 'x0 x2', 'x1^2', 'x1 x2', 'x2^2']

pd.DataFrame(data_poly, columns=pnf2.get_feature_names())
'''
    	1	x0	x1	x2	x0^2	x0 x1	x0 x2	x1^2	x1 x2	x2^2
    0	1.0	0.0	1.0	2.0	0.0	    0.0	    0.0	    1.0	    2.0	    4.0
    1	1.0	3.0	4.0	5.0	9.0	    12.0	15.0	16.0	20.0	25.0
    2	1.0	6.0	7.0	8.0	36.0	42.0	48.0	49.0	56.0	64.0
    3	1.0	9.0	10.011.081.0	90.0	99.0	100.0	110.0	121.0
'''

# degreeë¥¼ 5ë¡œ ëŠ˜ë¦¼
pnf3 = PolynomialFeatures(degree=5)
data_poly2 = pnf3.fit_transform(data)
data_poly2.shape    #(4, 56)
pnf3.get_feature_names()
# ['1','x0','x1','x2','x0^2','x0 x1','x0 x2','x1^2','x1 x2','x2^2','x0^3','x0^2 x1','x0^2 x2','x0 x1^2','x0 x1 x2','x0 x2^2',
#  'x1^3','x1^2 x2','x1 x2^2','x2^3','x0^4','x0^3 x1','x0^3 x2','x0^2 x1^2','x0^2 x1 x2','x0^2 x2^2','x0 x1^3','x0 x1^2 x2',
#  'x0 x1 x2^2','x0 x2^3','x1^4','x1^3 x2','x1^2 x2^2','x1 x2^3','x2^4','x0^5','x0^4 x1','x0^4 x2','x0^3 x1^2','x0^3 x1 x2',
#  'x0^3 x2^2','x0^2 x1^3','x0^2 x1^2 x2','x0^2 x1 x2^2','x0^2 x2^3','x0 x1^4','x0 x1^3 x2','x0 x1^2 x2^2','x0 x1 x2^3',
#  'x0 x2^4','x1^5','x1^4 x2','x1^3 x2^2','x1^2 x2^3','x1 x2^4','x2^5']

pnf = PolynomialFeatures(degree=100, include_bias=False)
X_train_poly_100 = pnf.fit_transform(X)
X_train_poly_100.shape  #(100, 100)
X.shape                 #(100, 1)

lr = LinearRegression()
lr.fit(X_train_poly_100, y)

X_new = np.linspace(-3,3,100).reshape(-1, 1)
X_new_poly_100 = pnf.transform(X_new)
pred_new_100 = lr.predict(X_new_poly_100)

plt.figure(figsize=(7,6))
plt.scatter(X, y)
plt.plot(X_new, pred_new_100, color='r', alpha=0.5)
plt.title('degree = 100')
plt.show()

#############################################################################################
####################### Boston Dataì™€ Polynomial Regression  ################################
#############################################################################################
pnf = PolynomialFeatures(degree=2, include_bias=False)
data = load_boston()

X, y = data['data'], data['target']
df = pd.DataFrame(X, columns=data['feature_names'])
df['MEDV'] = y

chas_df = pd.get_dummies(df['CHAS'])
chas_df.columns = ['CHAS_0', 'CHAS_1']

df2 = df.join(chas_df) #CHAS ì›í•«ì¸ì½”ë”© ì»¬ëŸ¼ì„ DFì— ì¶”ê°€
df2.drop(columns='CHAS', inplace=True) # ì›ë˜ ìˆë˜ CHASì»¬ëŸ¼ ì œê±°

y = df2['MEDV'] #target data
X = df2.drop(columns='MEDV')

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
scaler = StandardScaler()
X_trian_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

pnf = PolynomialFeatures(degree=2, include_bias=False)
X_train_scaled_poly = pnf.fit_transform(X_trian_scaled)     #(379, 13)
X_test_scaled_poly = pnf.transform(X_test_scaled)           #(379, 104)

lr3 = LinearRegression()
lr3.fit(X_train_scaled_poly, y_train)
pred_train3 = lr3.predict(X_train_scaled_poly)
pred_test3 = lr3.predict(X_test_scaled_poly)

print_regression_metrics(y_train, pred_train3, title='Train poly')
    #MSE : 4.09231630944325, RMSE : 2.0229474312110165, R2 = 0.952029059282025
print_regression_metrics(y_test, pred_test3, title='Test poly')
    #MSE : 31.957178742416264, RMSE : 5.653068082237845, R2 = 0.6088425475989248

df2.head()
    #      CRIM    ZN  INDUS    NOX     RM   AGE     DIS  RAD    TAX  PTRATIO       B  LSTAT  MEDV  CHAS_0  CHAS_1
    #0  0.00632  18.0   2.31  0.538  6.575  65.2  4.0900  1.0  296.0     15.3  396.90   4.98  24.0       1       0
    #1  0.02731   0.0   7.07  0.469  6.421  78.9  4.9671  2.0  242.0     17.8  396.90   9.14  21.6       1       0
    #2  0.02729   0.0   7.07  0.469  7.185  61.1  4.9671  2.0  242.0     17.8  392.83   4.03  34.7       1       0
    #3  0.03237   0.0   2.18  0.458  6.998  45.8  6.0622  3.0  222.0     18.7  394.63   2.94  33.4       1       0
    #4  0.06905   0.0   2.18  0.458  7.147  54.2  6.0622  3.0  222.0     18.7  396.90   5.33  36.2       1       0

'''
Ridge Regression
    - ì†ì‹¤í•¨ìˆ˜(loss fucntion)ì— ê·œì œí•­ìœ¼ë¡œ Î± * âˆ‘ (w_i)^2 (L2 Norm)ì„ ë”í•´ì¤€ë‹¤
    - Î± = 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê·œì œê°€ ì•½í•´ì§„ë‹¤(0ì¼ ê²½ìš° ì„ í˜• íšŒê·€ì™€ ë™ì¼)
    - Î±ê°€ ì»¤ì§ˆ ìˆ˜ë¡ ëª¨ë“  ê°€ì¤‘ì¹˜ê°€ ì‘ì•„ì ¸ ì…ë ¥ ë°ì´í„°ì˜ featureë“¤ ì¤‘ ì¤‘ìš”í•˜ì§€ ì•Šì€ featureì˜ outputì— ëŒ€í•œ ì˜í–¥ë ¥ì´ ì‘ì•„ì§
    - ì†ì‹¤í•¨ìˆ˜(w) = MSE(w) + Î± * (1/2) * âˆ‘(w_i)^2
'''
from sklearn.datasets import load_boston
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Ridge

X, y = load_boston(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# alpha = 1
ridge1 = Ridge(random_state=0) 
ridge1.fit(X_train_scaled, y_train)
pred_train1 = ridge1.predict(X_train_scaled)
pred_test1 = ridge1.predict(X_test_scaled)
print('alpha=1')
print_regression_metrics(y_train, pred_train1, title='Train')
print_regression_metrics(y_test, pred_test1, title="Test")
ridge1.coef_    #array([-0.96187481,  1.02775462, -0.06861144,  0.59814087, -1.77318401, 2.6205672 , -0.20466821, -2.96504904,  2.00091047, -1.85840697, -2.14955893,  0.75175979, -3.57350065])

#alpah=0.01
ridge1 = Ridge(alpha=0.01, random_state=0) 
ridge1.fit(X_train_scaled, y_train)
pred_train1 = ridge1.predict(X_train_scaled)
pred_test1 = ridge1.predict(X_test_scaled)
print('alpha=0.01')
print_regression_metrics(y_train, pred_train1, title='Train')
print_regression_metrics(y_test, pred_test1, title="Test")
ridge1.coef_    #array([-0.97090686,  1.04648351, -0.04074187,  0.59413006, -1.80840456, 2.61003017, -0.19830017, -3.00178921,  2.07939188, -1.93211252, -2.15735709,  0.75198861, -3.59010071])


#alpah=1000
ridge1 = Ridge(alpha=1000, random_state=0) 
ridge1.fit(X_train_scaled, y_train)
pred_train1 = ridge1.predict(X_train_scaled)
pred_test1 = ridge1.predict(X_test_scaled)
print('alpha=1000')
print_regression_metrics(y_train, pred_train1, title='Train')
print_regression_metrics(y_test, pred_test1, title="Test")
ridge1.coef_    #array([-0.44267768,  0.38220219, -0.51288178,  0.3335525 , -0.37129939, 1.25386598, -0.32729508, -0.06287806, -0.28302417, -0.47738562, -0.87977916,  0.4225767 , -1.16283877])



### GridSearchë¥¼ ì´ìš©í•œ ìµœì í™”ëœ alpha ì°¾ê¸°
from sklearn.model_selection import GridSearchCV
param = {"alpha":[0.01, 0.1, 1, 5, 10, 20, 30, 40, 100]}
ridge = Ridge(random_state=0)
gs = GridSearchCV(ridge, param, cv=4, scoring=['r2', 'neg_mean_squared_error'], refit='r2')

gs.fit(X_train_scaled, y_train)
result_df = pd.DataFrame(gs.cv_results_)
print(result_df.sort_values('rank_test_r2').head())
    #   mean_fit_time  std_fit_time  mean_score_time  ...  mean_test_neg_mean_squared_error std_test_neg_mean_squared_error rank_test_neg_mean_squared_error
    #5       0.001501      0.000501         0.001251  ...                        -23.363210                        5.542457                                2
    #4       0.001501      0.000500         0.001003  ...                        -23.352349                        5.376125                                1
    #6       0.000908      0.000584         0.000751  ...                        -23.435915                        5.597524                                4
    #3       0.001002      0.000708         0.000750  ...                        -23.393786                        5.196172                                3
    #7       0.000000      0.000000         0.003907  ...                        -23.546877                        5.607163                                8


'''
Lasso(Least Absolut Shrinkage and Selection Operator) Regression
    - ì†ì‹¤í•¨ìˆ˜ì— ê·œì œí•­ìœ¼ë¡œ ğ›¼âˆ‘|ğ‘¤ğ‘–|(L1 Norm)ë”í•œë‹¤.
    - Lasso íšŒê·€ì˜ ìƒëŒ€ì ìœ¼ë¡œ ëœ ì¤‘ìš”í•œ íŠ¹ì„±ì˜ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ìë™ìœ¼ë¡œ Feature Selectionì´ ëœë‹¤.
    - ì†ì‹¤í•¨ìˆ˜(ğ‘¤)=MSE(ğ‘¤)+ğ›¼âˆ‘|ğ‘¤ğ‘–|
'''

from sklearn.linear_model import Lasso

#alpha = 1
lasso = Lasso()
lasso.fit(X_train_scaled, y_train)
pred_train = lasso.predict(X_train_scaled)
pred_test = lasso.predict(X_test_scaled)
print_regression_metrics(y_train, pred_train, "alpha=1 Train")
print_regression_metrics(y_test, pred_test, "alpha=1 Test")

#alpha=10
lasso = Lasso(alpha=10, random_state=0) 
lasso.fit(X_train_scaled, y_train)
pred_train = lasso.predict(X_train_scaled)
pred_test = lasso.predict(X_test_scaled)
print_regression_metrics(y_train, pred_train, "alpha=10 Train")
print_regression_metrics(y_test, pred_test, "alpha=10 Test")




###Boston Datasetì„ ì´ìš©í•´ì„œ Ridge, Lasso(Polynomial Featuresë¡œ ì „ì²˜ë¦¬)###
from sklearn.linear_model import Ridge, Lasso, LinearRegression

alpha_lst = [0.01, 0.1, 1, 10, 100]
lr = LinearRegression()
lr.fit(X_train_scaled_poly, y_train)
pred_train_lr = lr.predict(X_train_scaled_poly)
pred_test_lr = lr.predict(X_test_scaled_poly)
print_regression_metrics(y_train, pred_train_lr, title="LinearRegression Train")
print_regression_metrics(y_test, pred_test_lr, title="LinearRegression Test")

#Ridgeì˜ alphaê°’ ë³€í™”ì— ë”°ë¥¸ R2ê°’ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ìƒì„±
ridge_train_metrics_list = []
ridge_test_metrics_list = []

for alpha in alpha_lst:
    ridge = Ridge(alpha = alpha, random_state=0)
    ridge.fit(X_train_scaled_poly, y_train)
    pred_train = ridge.predict(X_train_scaled_poly)
    pred_test = ridge.predict(X_test_scaled_poly)
    ridge_train_metrics_list.append(r2_score(y_train, pred_train))
    ridge_test_metrics_list.append(r2_score(y_test, pred_test))

ridge_df = pd.DataFrame({
    'alpha' : alpha_lst,
    'train' : ridge_test_metrics_list,
    'test' : ridge_test_metrics_list
})

print(ridge_df)

#Lassoì˜ alphaê°’ ë³€í™”ì— ë”°ë¼ì„œ R2ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
lasso_train_metrics_list = []
lasso_test_metrics_list = []

for alpha in alpha_lst:
    lasso = Lasso(alpha=alpha, random_state=0)
    lasso.fit(X_train_scaled_poly, y_train)
    pred_train = lasso.predict(X_train_scaled_poly)
    pred_test = lasso.predict(X_test_scaled_poly)
    lasso_train_metrics_list.append(r2_score(y_train, pred_train))
    lasso_test_metrics_list.append(r2_score(y_test, pred_test))

lasso_df = pd.DataFrame({
    "alpha":alpha_lst,
    "train":lasso_train_metrics_list,
    "test":lasso_test_metrics_list
})

print(lasso_df)



'''
ì—˜ë¼ìŠ¤í‹±ë„·
    - ë¦¿ì§€ì™€ ë¼ì˜ë¥¼ ì ˆì¶©í•œ ëª¨ë¸.
    - ê·œì œí•­ì— ë¦¿ì§€, íšŒê·€ ê·œì œí•­ì„ ë”í•´ì„œ ì¶”ê°€í•œë‹¤.
    - í˜¼í•©ë·°ìœ¨ ğ‘Ÿì„ ì‚¬ìš©í•´ í˜¼í•©ì •ë„ë¥¼ ì¡°ì ˆ
    - ğ‘Ÿ=0ì´ë©´ ë¦¿ì§€ì™€ ê°™ê³  ğ‘Ÿ=1ì´ë©´ ë¼ì˜ì™€ ê°™ë‹¤.
    - ì†ì‹¤í•¨ìˆ˜(ğ‘¤)=MSE(ğ‘¤)+ğ‘Ÿğ›¼âˆ‘|ğ‘¤ğ‘–|+ {(1âˆ’ğ‘Ÿ)/2} * ğ›¼âˆ‘(ğ‘¤_ğ‘–)^2
'''
from sklearn.linear_model import ElasticNet

elastic = ElasticNet(alpha=0.1, l1_ratio=0.5) #L1ê·œì œ(Lasso) ë¹„ìœ¨: 0.4, L2ê·œì œ(Ridge) ë¹„ìœ¨: 0.6
elastic.fit(X_train_scaled_poly, y_train)

pred_train = elastic.predict(X_train_scaled_poly)
pred_test = elastic.predict(X_test_scaled_poly)

print_regression_metrics(y_train, pred_train, title='Train')
print_regression_metrics(y_test, pred_test, title='Test')


'''
Summary
    - ì¼ë°˜ì ìœ¼ë¡œ ì„ í˜•íšŒê·€ì˜ ê²½ìš° ì–´ëŠì •ë„ ê·œì œê°€ ìˆëŠ” ê²½ìš°ê°€ ì„±ëŠ¥ì´ ì¢‹ë‹¤.
    - ê¸°ë³¸ì ìœ¼ë¡œ ë¦¿ì§€ë¥¼ ì‚¬ìš©í•œë‹¤.
    - Targetì— ì˜í–¥ì„ ì£¼ëŠ” Featureê°€ ëª‡ ê°œë¿ì¼ ê²½ìš° íŠ¹ì„±ì˜ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ì£¼ëŠ” ë¼ì˜ ì‚¬ìš©í•œë‹¤.
    - íŠ¹ì„± ìˆ˜ê°€ í•™ìŠµ ìƒ˜í”Œ ìˆ˜ ë³´ë‹¤ ë§ê±°ë‚˜ featureê°„ì— ì—°ê´€ì„±ì´ ë†’ì„ ë•ŒëŠ” ì—˜ë¼ìŠ¤í‹±ë„·ì„ ì‚¬ìš©í•œë‹¤.
'''